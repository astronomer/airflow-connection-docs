
### Module - Databricks Connection + Operators

**Before you start**

- Have a Databricks Account (with permissions to create Access Tokens or have an Access Token ready)

**Gather Credentials**

- [ ]  Obtain Databricks `Host Endpoint`

<aside>
ðŸ’¡ If you login to Databricks via a web browser, this parameter is in the URL. So if your URL was: `https://adb-123456789.10.azuredatabricks.net/?o=557863513108#setting/account` then your `Host Endpoint`would be `https://adb-123456789.10.azuredatabricks.net`

</aside>

- [ ]  Create a new `Access Token`

<aside>
ðŸ’¡ If you login to Databricks via a web browser, this parameter can be generated by navigating to `Settings >> User Setting` and clicking `Generate New Token`

</aside>

**Add Databricks to your Airflow environment**

- [ ]  Add the following lines to your `requirements.txt` file:

```
apache-airflow-providers-databricks
astronomer-providers[databricks]
```

<aside>
ðŸ’¡ If you are running Airflow locally, youâ€™ll need to restart the project so that the Databricks provider is installed. You can restart your project using `astro dev restart`

</aside>

**Create Connection**

- [ ]  Create a connection in one of the following ways:
    - Add a Databricks Connection in your Dockerfile, a Secrets Backend, or via [Env Variables](https://docs.astronomer.io/astro/environment-variables)

        ```bash
        export AIRFLOW_CONN_DATABRICKS_DEFAULT='databricks://<HOST-ENDPOINT>?token=<ACCESS-TOKEN>'
        ```

    - **OR** go to Admin > Connections in the Airflow UI, fill in `Connection ID` field (`databricks_default`), select `Databricks` as a connection type, in the `Extra` field add:

        ```json
        {"host": "<HOST-ENDPOINT>", "token": "<ACCESS-TOKEN>"}
        ```

    - For more information on the Databricks connection in Airflow, be sure to review the Airflow OSS documentation

        [Databricks Connection - apache-airflow-providers-databricks Documentation](https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/connections/databricks.html)


**Test the Connection**

- [ ]  Run Test DAG:

```python
import sys
import logging
from datetime import datetime
from airflow.providers.databricks.hooks.databricks import DatabricksHook
from airflow.decorators import dag, task

@dag(schedule=None, start_date=datetime(2021, 12, 1), catchup=False)
def databricks_example():

    @task(task_id='test_connection')
    def test_connection() -> [bool, str]:
        hook = DatabricksHook()
        try:
            hook._do_api_call(endpoint_info=('GET', 'api/2.0/jobs/list'))
            status = True
            message = 'Connection successfully tested'
        except Exception as e:
            status = False
            message = str(e)

        if status is False:
            logging.info(message)
            sys.exit()
        else:
            logging.info(message)
            return status, message

    test_connection()

dag = databricks_example()
```

**Checkpoint**

You now have

- tested the Databricks connection in Airflow
- and have utilized the `DatabricksHook`

**Next Steps**

- Integrate the new Airflow Connection with your Use Case Workload
- [Review other Databricks Operators](https://registry.astronomer.io/modules/?query=Databricks&page=1) in the Astronomer Registry
- Reference


    ### Databricks

    - [GitHub](https://github.com/astronomer/cs-tutorial-databricks): *Proof-of-concept that shows all current Databricks Operators and Hook sub-methods. The readme on this repository will walk the user through how to set up a default databricks connection. Proof-of-concept uses Databricks on Azure, but the same principles apply across various infrastructures.*
    - [Orchestrating Databricks Jobs with Airflow (Astronomer Guide)](https://www.astronomer.io/guides/airflow-databricks) : *Showcases the two main Databricks operators, as well as how and when to use them. Guide also showcases referencing various Databricks elements in Airflow (clusters, jobs, job runs, etc.)*
    - [Databricks Hook & Operator | Astronomer Registry](https://registry.astronomer.io/providers/databricks/)
    - [Example Databricks DAGs | Astronomer Registry](https://registry.astronomer.io/dags?query=databricks&page=1)
    - [Databricks blog post](https://databricks.com/blog/2017/07/19/integrating-apache-airflow-with-databricks.html) on integrating with Airflow
    - [Apache docs](https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/search.html?q=databricks&check_keywords=yes&area=default) on Databricks provider package
    - [Orchestrate Databricks jobs with Airflow | Astronomer Tutorial](https://docs.astronomer.io/learn/airflow-databricks)
